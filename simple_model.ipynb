{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from data_loader import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers += [nn.Linear(input_dim, 200)]\n",
    "        layers += [nn.ReLU(True)]\n",
    "        layers += [nn.Linear(200, 100)]\n",
    "        layers += [nn.ReLU(True)]\n",
    "        layers += [nn.Linear(100, 50)]\n",
    "        layers += [nn.ReLU(True)]\n",
    "        layers += [nn.Linear(50, 10)]\n",
    "        layers += [nn.ReLU(True)]\n",
    "        layers += [nn.Linear(10, 5)]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        layers = []\n",
    "        layers += [nn.Linear(5, 10)]\n",
    "        layers += [nn.ReLU(True)]\n",
    "        layers += [nn.Linear(10, 50)]\n",
    "        layers += [nn.ReLU(True)]\n",
    "        layers += [nn.Linear(50, 100)]\n",
    "        layers += [nn.ReLU(True)]\n",
    "        layers += [nn.Linear(100, 200)]\n",
    "        layers += [nn.ReLU(True)]\n",
    "        layers += [nn.Linear(200, input_dim)]\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = 5 * 24 * 60\n",
    "data = get_loader(timeframe=timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input dimention\n",
    "lr = 1e-3\n",
    "ds = iter(data)\n",
    "input_dim = next(ds).shape[2]\n",
    "# inizialise all needed parameters\n",
    "model = SimpleAutoencoder(input_dim=input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=50400, out_features=200, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=50, out_features=10, bias=True)\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Linear(in_features=10, out_features=5, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=10, out_features=50, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=100, out_features=200, bias=True)\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Linear(in_features=200, out_features=50400, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch_num = 1\n",
    "# model = model.float\n",
    "for epoch in range(epoch_num):\n",
    "    for i, x in enumerate(data):\n",
    "        # forward\n",
    "        model.train()\n",
    "        x = x.float()\n",
    "        y = model(x).float()\n",
    "        loss = criterion(y, x)\n",
    "#         MSE_loss = nn.CrossEntropyLoss()(y, x)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # ------- log output ---------\n",
    "    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, epoch_num, loss.data, 0))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i, x in enumerate(data):\n",
    "    x = x.float()\n",
    "    y = model(x[0])\n",
    "    loss = criterion(y,x[0])\n",
    "    print(criterion(y,x[0]))\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test soll folgendes ausgeben\n",
    "# 10 h√∂chsten Werte mit Datum!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_autoencoder = pd.read_csv('data/autoencoder_v1_PAH3DEEUR_1 Min_Bid_2008.10.21_2018.10.27', header=None)\n",
    "df = pd.read_csv('data/PAH3DEEUR_1 Min_Bid_2008.10.21_2018.10.27.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_autoencoder.shape)\n",
    "print(df.shape)\n",
    "dp = get_loader(batch_size=1, timeframe=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i, x in enumerate(dp):\n",
    "    # add value to pandas \n",
    "#     x = x.unsqueeze(1)\n",
    "    result.append(criterion(x.float(), model(x.float())).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r[0].nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[513047:513047+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513047</th>\n",
       "      <td>1.591862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513048</th>\n",
       "      <td>1.561770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513049</th>\n",
       "      <td>1.511238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513050</th>\n",
       "      <td>1.507709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513057</th>\n",
       "      <td>1.511746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513058</th>\n",
       "      <td>1.563944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513059</th>\n",
       "      <td>1.602898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513060</th>\n",
       "      <td>1.603214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513061</th>\n",
       "      <td>1.593110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513062</th>\n",
       "      <td>1.617410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513063</th>\n",
       "      <td>1.614714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513064</th>\n",
       "      <td>1.633250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513065</th>\n",
       "      <td>1.712679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513066</th>\n",
       "      <td>1.680416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513067</th>\n",
       "      <td>1.703335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513068</th>\n",
       "      <td>1.696168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513069</th>\n",
       "      <td>1.691750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513070</th>\n",
       "      <td>1.721148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513071</th>\n",
       "      <td>1.712173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513072</th>\n",
       "      <td>1.718446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513073</th>\n",
       "      <td>1.656246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513074</th>\n",
       "      <td>1.685177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513075</th>\n",
       "      <td>1.673417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513076</th>\n",
       "      <td>1.661181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513077</th>\n",
       "      <td>1.726023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513078</th>\n",
       "      <td>1.785633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513079</th>\n",
       "      <td>1.884609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513080</th>\n",
       "      <td>1.871319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513081</th>\n",
       "      <td>1.894675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513082</th>\n",
       "      <td>1.906546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521603</th>\n",
       "      <td>2.066225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521604</th>\n",
       "      <td>2.049951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521605</th>\n",
       "      <td>2.088154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521606</th>\n",
       "      <td>2.085004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521607</th>\n",
       "      <td>2.079786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521608</th>\n",
       "      <td>2.078404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521609</th>\n",
       "      <td>2.093457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521610</th>\n",
       "      <td>2.114874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521611</th>\n",
       "      <td>2.155720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521612</th>\n",
       "      <td>2.176913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521613</th>\n",
       "      <td>2.135619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521614</th>\n",
       "      <td>2.115486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521615</th>\n",
       "      <td>2.090827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521616</th>\n",
       "      <td>2.079058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521617</th>\n",
       "      <td>2.101845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521618</th>\n",
       "      <td>2.080958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521619</th>\n",
       "      <td>2.129033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521620</th>\n",
       "      <td>2.172595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521621</th>\n",
       "      <td>2.166650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521622</th>\n",
       "      <td>2.175978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521623</th>\n",
       "      <td>2.194724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521624</th>\n",
       "      <td>2.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521625</th>\n",
       "      <td>2.093945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521626</th>\n",
       "      <td>2.018365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521627</th>\n",
       "      <td>1.948023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521628</th>\n",
       "      <td>1.813259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521629</th>\n",
       "      <td>1.744336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521630</th>\n",
       "      <td>1.675475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521631</th>\n",
       "      <td>1.620530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521632</th>\n",
       "      <td>1.519791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "513047  1.591862\n",
       "513048  1.561770\n",
       "513049  1.511238\n",
       "513050  1.507709\n",
       "513057  1.511746\n",
       "513058  1.563944\n",
       "513059  1.602898\n",
       "513060  1.603214\n",
       "513061  1.593110\n",
       "513062  1.617410\n",
       "513063  1.614714\n",
       "513064  1.633250\n",
       "513065  1.712679\n",
       "513066  1.680416\n",
       "513067  1.703335\n",
       "513068  1.696168\n",
       "513069  1.691750\n",
       "513070  1.721148\n",
       "513071  1.712173\n",
       "513072  1.718446\n",
       "513073  1.656246\n",
       "513074  1.685177\n",
       "513075  1.673417\n",
       "513076  1.661181\n",
       "513077  1.726023\n",
       "513078  1.785633\n",
       "513079  1.884609\n",
       "513080  1.871319\n",
       "513081  1.894675\n",
       "513082  1.906546\n",
       "...          ...\n",
       "521603  2.066225\n",
       "521604  2.049951\n",
       "521605  2.088154\n",
       "521606  2.085004\n",
       "521607  2.079786\n",
       "521608  2.078404\n",
       "521609  2.093457\n",
       "521610  2.114874\n",
       "521611  2.155720\n",
       "521612  2.176913\n",
       "521613  2.135619\n",
       "521614  2.115486\n",
       "521615  2.090827\n",
       "521616  2.079058\n",
       "521617  2.101845\n",
       "521618  2.080958\n",
       "521619  2.129033\n",
       "521620  2.172595\n",
       "521621  2.166650\n",
       "521622  2.175978\n",
       "521623  2.194724\n",
       "521624  2.152800\n",
       "521625  2.093945\n",
       "521626  2.018365\n",
       "521627  1.948023\n",
       "521628  1.813259\n",
       "521629  1.744336\n",
       "521630  1.675475\n",
       "521631  1.620530\n",
       "521632  1.519791\n",
       "\n",
       "[188 rows x 1 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r[df_r > 1.5].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
