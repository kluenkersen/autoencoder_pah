{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from data_loader import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: normalizer for get_loader between -1 and 1\n",
    "dl = get_loader(batch_size=128, timeframe=60)\n",
    "\n",
    "# i = 0\n",
    "# ds = iter(dl)\n",
    "# for i in range(10):\n",
    "#     print(ds.next()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from data_loader import get_loader\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim=3, n_gmm=2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # inizialize encoder\n",
    "        layers = []\n",
    "        layer_size = input_size\n",
    "        # das ist noch super haessliche muss noch angepasst werden zwischen 10-20 \n",
    "        # oder 30 soll es liegen abhaengig vom input\n",
    "        while ((layer_size) >= 20):\n",
    "            layers += [nn.Linear(layer_size, layer_size / 2)]\n",
    "            layers += [nn.Tanh()]\n",
    "            layer_size = layer_size / 2\n",
    "        layers += [nn.Linear(layer_size, 1)]\n",
    "        \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # inizialize decoder\n",
    "        layers = []\n",
    "        layers += [nn.Linear(1, layer_size)]\n",
    "        while (layer_size != input_size):\n",
    "            layers += [nn.Tanh()]\n",
    "            layers += [nn.Linear(layer_size, layer_size * 2)]\n",
    "            layer_size = layer_size * 2\n",
    "            # auch super haesslich muss ich nochmal saubber loese geht erstmal fuer 420\n",
    "            if(layer_size == 104):\n",
    "                layer_size += 1\n",
    "            \n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        \n",
    "        layers = []\n",
    "        layers += [nn.Linear(latent_dim,10)]\n",
    "        layers += [nn.Tanh()]        \n",
    "        layers += [nn.Dropout(p=0.5)]        \n",
    "        layers += [nn.Linear(10,n_gmm)]\n",
    "        layers += [nn.Softmax(dim=1)]\n",
    "\n",
    "\n",
    "        self.estimation = nn.Sequential(*layers)\n",
    "\n",
    "        self.register_buffer(\"phi\", torch.zeros(n_gmm))\n",
    "        self.register_buffer(\"mu\", torch.zeros(n_gmm,latent_dim))\n",
    "        self.register_buffer(\"cov\", torch.zeros(n_gmm,latent_dim,latent_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "# copy paste has to be done again by me\n",
    "\n",
    "    def relative_euclidean_distance(self, a, b):\n",
    "        return (a-b).norm(2, dim=1) / a.norm(2, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        enc = self.encoder(x)\n",
    "        dec = self.decoder(enc)\n",
    "        rec_cosine = F.cosine_similarity(x, dec, dim=1)\n",
    "        rec_euclidean = self.relative_euclidean_distance(x, dec)\n",
    "        z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n",
    "        gamma = self.estimation(z)\n",
    "\n",
    "        return enc, dec, z, gamma\n",
    "\n",
    "    def compute_gmm_params(self, z, gamma):\n",
    "        N = gamma.size(0)\n",
    "        # K\n",
    "        sum_gamma = torch.sum(gamma, dim=0)\n",
    "        # K\n",
    "        phi = (sum_gamma / N)\n",
    "        self.phi = phi.data\n",
    "\n",
    " \n",
    "        # K x D\n",
    "        mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n",
    "        self.mu = mu.data\n",
    "        # z = N x D\n",
    "        # mu = K x D\n",
    "        # gamma N x K\n",
    "        # z_mu = N x K x D\n",
    "        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n",
    "        # z_mu_outer = N x K x D x D\n",
    "        z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "        # K x D x D\n",
    "        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim = 0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        self.cov = cov.data\n",
    "\n",
    "        return phi, mu, cov\n",
    "        \n",
    "    def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n",
    "        if phi is None:\n",
    "            phi = to_var(self.phi)\n",
    "        if mu is None:\n",
    "            mu = to_var(self.mu)\n",
    "        if cov is None:\n",
    "            cov = to_var(self.cov)\n",
    "        k, D, _ = cov.size()\n",
    "        z_mu = (z.unsqueeze(1)- mu.unsqueeze(0))\n",
    "        cov_inverse = []\n",
    "        det_cov = []\n",
    "        cov_diag = 0\n",
    "        eps = 1e-12\n",
    "        for i in range(k):\n",
    "            # K x D x D\n",
    "            cov_k = cov[i] + to_var(torch.eye(D)*eps)\n",
    "            cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n",
    "\n",
    "            #det_cov.append(np.linalg.det(cov_k.data.cpu().numpy()* (2*np.pi)))\n",
    "            det_cov.append((Cholesky.apply(cov_k.cpu() * (2*np.pi)).diag().prod()).unsqueeze(0))\n",
    "            cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n",
    "        # K x D x D\n",
    "        cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "        # K\n",
    "        det_cov = torch.cat(det_cov).cuda()\n",
    "        #det_cov = to_var(torch.from_numpy(np.float32(np.array(det_cov))))\n",
    "        # N x K\n",
    "        exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n",
    "        # for stability (logsumexp)\n",
    "        max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n",
    "        exp_term = torch.exp(exp_term_tmp - max_val)\n",
    "        # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (det_cov).unsqueeze(0), dim = 1) + eps)\n",
    "        sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt(det_cov)).unsqueeze(0), dim = 1) + eps)\n",
    "        # sample_energy = -max_val.squeeze() - torch.log(torch.sum(phi.unsqueeze(0) * exp_term / (torch.sqrt((2*np.pi)**D * det_cov)).unsqueeze(0), dim = 1) + eps)\n",
    "        if size_average:\n",
    "            sample_energy = torch.mean(sample_energy)\n",
    "\n",
    "        return sample_energy, cov_diag\n",
    "\n",
    "\n",
    "    def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n",
    "\n",
    "        recon_error = torch.mean((x - x_hat) ** 2)\n",
    "        phi, mu, cov = self.compute_gmm_params(z, gamma)\n",
    "        sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n",
    "        loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n",
    "\n",
    "        return loss, sample_energy, recon_error, cov_diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = Autoencoder(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "105\n",
      "52\n",
      "26\n",
      "13\n",
      "6\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "k = 420\n",
    "for i in range(20):\n",
    "    k = k / 2\n",
    "    print (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
